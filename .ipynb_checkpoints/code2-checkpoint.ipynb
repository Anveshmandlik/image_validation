{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anves\\vfs_project\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from deepface import DeepFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Image Dimension Validation\n",
    "def validate_dimensions(image_path, required_width=413, required_height=531):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return False, \"Invalid image file\"\n",
    "    height, width, _ = image.shape\n",
    "    return (\n",
    "        width == required_width and height == required_height,\n",
    "        f\"Dimensions should be {required_width}x{required_height} pixels\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_background(image_path, threshold=180):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return False, \"Invalid image file\"\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a mask for the background (assuming the background is the largest connected component)\n",
    "    _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    background_mask = np.zeros_like(gray)\n",
    "    cv2.drawContours(background_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Calculate the percentage of white pixels in the background\n",
    "    white_pixels = np.sum((gray > threshold) & (background_mask == 255))\n",
    "    total_pixels = np.sum(background_mask == 255)\n",
    "    white_percentage = (white_pixels / total_pixels) * 100\n",
    "    print(f\"White Percentage: {white_percentage:.2f}%\")\n",
    "\n",
    "    # Check if the background is predominantly white\n",
    "    return white_percentage > 80, \"Background should be predominantly white\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Face Detection and Positioning\n",
    "def validate_face_positioning(image_path):\n",
    "    detector = MTCNN()\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    if image is None:\n",
    "        return False, \"Invalid image file\"\n",
    "\n",
    "    results = detector.detect_faces(image)\n",
    "    if not results:\n",
    "        return False, \"No face detected\"\n",
    "\n",
    "    face = results[0]\n",
    "    keypoints = face[\"keypoints\"]\n",
    "    height, width, _ = image.shape\n",
    "    face_height = face[\"box\"][3]\n",
    "\n",
    "    # Check if shoulders are visible\n",
    "    if face_height > 0.7 * height:\n",
    "        return False, \"Shoulders not visible\"\n",
    "\n",
    "    # Check if ears are unobstructed\n",
    "    if not all(keypoints.values()):\n",
    "        return False, \"Ears obstructed\"\n",
    "\n",
    "    return True, \"Face positioning valid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Expression and Accessories Validation\n",
    "# Emotion validation\n",
    "def validate_emotion(image_path):\n",
    "    try:\n",
    "        # Analyze emotion\n",
    "        attributes = DeepFace.analyze(\n",
    "            img_path=image_path, actions=[\"emotion\"], enforce_detection=False\n",
    "        )\n",
    "\n",
    "        # Handle multiple faces\n",
    "        if isinstance(attributes, list):\n",
    "            # Use the first face in the list\n",
    "            attributes = attributes[0]\n",
    "\n",
    "        # Check for neutral expression\n",
    "        if attributes[\"emotion\"][\"neutral\"] < 0.5:\n",
    "            return False, \"Expression not neutral\"\n",
    "\n",
    "        return True, \"Expression is neutral\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Emotion validation failed: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Validation: True Expression is neutral\n"
     ]
    }
   ],
   "source": [
    "result, message = validate_emotion(image_path)\n",
    "print(\"Emotion Validation:\", result, message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def validate_head_covering(image_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return False, \"Invalid image file\"\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Load Haar Cascade for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "        )\n",
    "\n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return False, \"No face detected\"\n",
    "\n",
    "        # Get the first detected face\n",
    "        (x, y, w, h) = faces[0]\n",
    "\n",
    "        # Check the aspect ratio of the face\n",
    "        aspect_ratio = w / h\n",
    "        if aspect_ratio < 0.7 or aspect_ratio > 1.3:  # Adjust thresholds as needed\n",
    "            return False, \"Head covering detected (abnormal aspect ratio)\"\n",
    "\n",
    "        # Analyze the upper region of the face for head coverings\n",
    "        upper_face = gray[y : y + h // 4, x : x + w]  # Upper 25% of the face\n",
    "        upper_face_mean = upper_face.mean()\n",
    "\n",
    "        if upper_face_mean < 100:  # Adjust threshold as needed\n",
    "            return False, \"Head covering detected (dark upper region)\"\n",
    "\n",
    "        return True, \"No head covering detected\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Head covering validation failed: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (False, 'Dimensions should be 413x531 pixels')\n",
      "White Percentage: 99.84%\n",
      "Background: (np.True_, 'Background should be predominantly white')\n",
      "Face Positioning: (True, 'Face positioning valid')\n",
      "Emotion Validation: True Expression is neutral\n",
      "Glasses Validation: False Glasses validation failed: Invalid action passed ('glasses')). Valid actions are `emotion`, `age`, `gender`, `race`.\n",
      "Head Covering Validation: False Head covering detected (dark upper region)\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "image_path = image_path = \"C:\\\\Users\\\\anves\\\\Downloads\\\\test2.jpg\"\n",
    "print(\"Dimensions:\", validate_dimensions(image_path))\n",
    "print(\"Background:\", validate_background(image_path))\n",
    "print(\"Face Positioning:\", validate_face_positioning(image_path))\n",
    "result, message = validate_emotion(image_path)\n",
    "print(\"Emotion Validation:\", result, message)\n",
    "\n",
    "head_covering_result, head_covering_message = validate_head_covering(image_path)\n",
    "print(\"Head Covering Validation:\", head_covering_result, head_covering_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: C:\\Users\\anves\\Downloads\\picture.jpg\n"
     ]
    }
   ],
   "source": [
    "image_path = \"C:\\\\Users\\\\anves\\\\Downloads\\\\picture.jpg\"\n",
    "print(\"Image Path:\", image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
