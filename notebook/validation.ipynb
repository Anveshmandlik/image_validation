{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anves\\vfs_project\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from deepface import DeepFace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Image Dimension Validation\n",
    "def validate_dimensions(image_path, required_width=413, required_height=531):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return False, \"Invalid image file\"\n",
    "    height, width, _ = image.shape\n",
    "    return (\n",
    "        width == required_width and height == required_height,\n",
    "        f\"Dimensions should be {required_width}x{required_height} pixels\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_background(image_path, threshold=180):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return False, \"Invalid image file\"\n",
    "\n",
    "    # Converting to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a mask for the background (assuming the background is the largest connected component)\n",
    "    _, binary = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    background_mask = np.zeros_like(gray)\n",
    "    cv2.drawContours(background_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Calculate the percentage of white pixels in the background\n",
    "    white_pixels = np.sum((gray > threshold) & (background_mask == 255))\n",
    "    total_pixels = np.sum(background_mask == 255)\n",
    "    white_percentage = (white_pixels / total_pixels) * 100\n",
    "    print(f\"White Percentage: {white_percentage:.2f}%\")\n",
    "\n",
    "    # Check if the background is predominantly white\n",
    "    return white_percentage > 80, \"Background should be predominantly white\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Face Detection and Positioning\n",
    "def validate_face_positioning(image_path):\n",
    "    detector = MTCNN()\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    if image is None:\n",
    "        return False, \"Invalid image file\"\n",
    "\n",
    "    results = detector.detect_faces(image)\n",
    "    if not results:\n",
    "        return False, \"No face detected\"\n",
    "\n",
    "    face = results[0]\n",
    "    keypoints = face[\"keypoints\"]\n",
    "    height, width, _ = image.shape\n",
    "    face_height = face[\"box\"][3]\n",
    "\n",
    "    # Check if shoulders are visible\n",
    "    if face_height > 0.7 * height:\n",
    "        return False, \"Shoulders not visible\"\n",
    "\n",
    "    # Check if ears are unobstructed\n",
    "    if not all(keypoints.values()):\n",
    "        return False, \"Ears obstructed\"\n",
    "\n",
    "    return True, \"Face positioning valid\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "# Emotion validation\n",
    "def validate_emotion(image_path):\n",
    "    try:\n",
    "        print(f\"Starting emotion validation on: {image_path}\")\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(image_path):\n",
    "            return False, \"Image file not found\"\n",
    "\n",
    "        # Analyze emotion\n",
    "        attributes = DeepFace.analyze(\n",
    "            img_path=image_path, actions=[\"emotion\"], enforce_detection=False\n",
    "        )\n",
    "\n",
    "        print(f\"DeepFace analysis completed\")\n",
    "\n",
    "        # Handle multiple faces\n",
    "        if isinstance(attributes, list):\n",
    "            if not attributes:\n",
    "                return False, \"No face detected for emotion analysis\"\n",
    "            # Use the first face in the list\n",
    "            attributes = attributes[0]\n",
    "\n",
    "        # Debug the emotion scores\n",
    "        emotion_scores = attributes[\"emotion\"]\n",
    "        print(f\"Emotion scores: {emotion_scores}\")\n",
    "\n",
    "        # Check for neutral expression\n",
    "        if emotion_scores[\"neutral\"] < 0.5:\n",
    "            dominant_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "            return False, f\"Expression not neutral (detected: {dominant_emotion})\"\n",
    "\n",
    "        return True, \"Expression is neutral\"\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in validate_emotion: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return False, f\"Emotion validation failed: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting emotion validation on: C:\\Users\\anves\\Downloads\\test2.jpg\n",
      "DeepFace analysis completed\n",
      "Emotion scores: {'angry': np.float32(0.583719), 'disgust': np.float32(4.4413217e-05), 'fear': np.float32(1.5197457), 'happy': np.float32(8.929203), 'sad': np.float32(4.2673764), 'surprise': np.float32(0.0028152522), 'neutral': np.float32(84.697105)}\n",
      "Emotion Validation: True Expression is neutral\n"
     ]
    }
   ],
   "source": [
    "result, message = validate_emotion(image_path)\n",
    "print(\"Emotion Validation:\", result, message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def validate_head_covering(image_path):\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return False, \"Invalid image file\"\n",
    "\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Load Haar Cascade for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "        )\n",
    "\n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return False, \"No face detected\"\n",
    "\n",
    "        # Get the first detected face\n",
    "        (x, y, w, h) = faces[0]\n",
    "\n",
    "        # Check the aspect ratio of the face\n",
    "        aspect_ratio = w / h\n",
    "        if aspect_ratio < 0.7 or aspect_ratio > 1.3:  # Adjust thresholds as needed\n",
    "            return False, \"Head covering detected (abnormal aspect ratio)\"\n",
    "\n",
    "        # Analyze the upper region of the face for head coverings\n",
    "        upper_face = gray[y : y + h // 4, x : x + w]  # Upper 25% of the face\n",
    "        upper_face_mean = upper_face.mean()\n",
    "\n",
    "        if upper_face_mean < 100:  # Adjust threshold as needed\n",
    "            return False, \"Head covering detected (dark upper region)\"\n",
    "\n",
    "        return True, \"No head covering detected\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Head covering validation failed: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (False, 'Dimensions should be 413x531 pixels')\n",
      "White Percentage: 99.84%\n",
      "Background: (np.True_, 'Background should be predominantly white')\n",
      "Face Positioning: (True, 'Face positioning valid')\n",
      "Starting emotion validation on: C:\\Users\\anves\\Downloads\\test2.jpg\n",
      "DeepFace analysis completed\n",
      "Emotion scores: {'angry': np.float32(0.583719), 'disgust': np.float32(4.4413217e-05), 'fear': np.float32(1.5197457), 'happy': np.float32(8.929203), 'sad': np.float32(4.2673764), 'surprise': np.float32(0.0028152522), 'neutral': np.float32(84.697105)}\n",
      "Emotion Validation: True Expression is neutral\n",
      "Head Covering Validation: False Head covering detected (dark upper region)\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "image_path = image_path = \"C:\\\\Users\\\\anves\\\\Downloads\\\\test2.jpg\"\n",
    "print(\"Dimensions:\", validate_dimensions(image_path))\n",
    "print(\"Background:\", validate_background(image_path))\n",
    "print(\"Face Positioning:\", validate_face_positioning(image_path))\n",
    "result, message = validate_emotion(image_path)\n",
    "print(\"Emotion Validation:\", result, message)\n",
    "head_covering_result, head_covering_message = validate_head_covering(image_path)\n",
    "print(\"Head Covering Validation:\", head_covering_result, head_covering_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Path: C:\\Users\\anves\\Downloads\\picture.jpg\n"
     ]
    }
   ],
   "source": [
    "image_path = \"C:\\\\Users\\\\anves\\\\Downloads\\\\picture.jpg\"\n",
    "print(\"Image Path:\", image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (False, 'Dimensions should be 413x531 pixels')\n",
      "White Percentage: 99.80%\n",
      "Background: (np.True_, 'Background should be predominantly white')\n",
      "Face Positioning: (True, 'Face positioning valid')\n",
      "Starting emotion validation on: C:\\Users\\anves\\Downloads\\testimage1.jpg\n",
      "DeepFace analysis completed\n",
      "Emotion scores: {'angry': np.float32(98.49446), 'disgust': np.float32(0.016209887), 'fear': np.float32(0.21796107), 'happy': np.float32(0.00048769385), 'sad': np.float32(0.27402943), 'surprise': np.float32(0.7810714), 'neutral': np.float32(0.2157687)}\n",
      "Emotion Validation: False Expression not neutral (detected: angry)\n",
      "Head Covering Validation: True No head covering detected\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "image_path = image_path = \"C:\\\\Users\\\\anves\\\\Downloads\\\\testimage1.jpg\"\n",
    "print(\"Dimensions:\", validate_dimensions(image_path))\n",
    "print(\"Background:\", validate_background(image_path))\n",
    "print(\"Face Positioning:\", validate_face_positioning(image_path))\n",
    "result, message = validate_emotion(image_path)\n",
    "print(\"Emotion Validation:\", result, message)\n",
    "head_covering_result, head_covering_message = validate_head_covering(image_path)\n",
    "print(\"Head Covering Validation:\", head_covering_result, head_covering_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: (False, 'Invalid image file')\n",
      "Background: (False, 'Invalid image file')\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDimensions:\u001b[39m\u001b[33m\"\u001b[39m, validate_dimensions(image_path))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBackground:\u001b[39m\u001b[33m\"\u001b[39m, validate_background(image_path))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFace Positioning:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mvalidate_face_positioning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m result, message = validate_emotion(image_path)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmotion Validation:\u001b[39m\u001b[33m\"\u001b[39m, result, message)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mvalidate_face_positioning\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_face_positioning\u001b[39m(image_path):\n\u001b[32m      3\u001b[39m     detector = MTCNN()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mInvalid image file\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "image_path = image_path = \"C:\\\\Users\\\\anves\\\\Downloads\\\\closeup.jpg\"\n",
    "print(\"Dimensions:\", validate_dimensions(image_path))\n",
    "print(\"Background:\", validate_background(image_path))\n",
    "print(\"Face Positioning:\", validate_face_positioning(image_path))\n",
    "result, message = validate_emotion(image_path)\n",
    "print(\"Emotion Validation:\", result, message)\n",
    "head_covering_result, head_covering_message = validate_head_covering(image_path)\n",
    "print(\"Head Covering Validation:\", head_covering_result, head_covering_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
